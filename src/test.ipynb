{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "from gsm_dataset import GSMDataset, gsm_collate, gsm_prompt, sample\n",
    "from biscuit import Biscuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = None # Add checkpoint path\n",
    "biscuit_model = Biscuit()\n",
    "biscuit_model.model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GSMDataset()\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "example_size = int(0.02 * len(dataset)) # reserve some data for few shot prompting\n",
    "test_size = len(dataset) - train_size - example_size\n",
    "\n",
    "train_dataset, example_dataset, test_dataset = random_split(dataset, [train_size, example_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=gsm_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=gsm_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_MAX_LENGTH = 6\n",
    "segments, keep_indices_lst = next(iter(train_loader))\n",
    "examples = sample(example_dataset, num_samples=4)\n",
    "prompt = gsm_prompt(examples)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step 0: just process the first segment without decoding the next token\n",
    "    for seg in segments[0]:\n",
    "        print(seg)\n",
    "    first_segment = [prompt + segment for segment in segments[0]]\n",
    "    inputs = biscuit_model.tokenizer(first_segment, return_tensors=\"pt\", padding=True).to(biscuit_model.device)\n",
    "    outputs = biscuit_model.model(**inputs)\n",
    "    kv_cache = outputs.past_key_values\n",
    "    attn_mask = inputs.attention_mask\n",
    "\n",
    "    # continuous CoT loop: produce CoT -> use it to predict next segment -> repeat\n",
    "    for segment, keep_indices in zip(segments[1:], keep_indices_lst):\n",
    "        # Step 1: drop sequences that are done\n",
    "        kv_cache.batch_select_indices(keep_indices)\n",
    "        attn_mask = attn_mask[keep_indices]\n",
    "        batch_size = keep_indices.shape[0]\n",
    "        attn_ones = torch.ones(batch_size, 1, dtype=int).to(biscuit_model.device)\n",
    "\n",
    "\n",
    "        # Step 2: then autoregressively predict a continuous chain of thought sequence\n",
    "        last_hidden_state = None\n",
    "        k = np.random.randint(1, COT_MAX_LENGTH + 1) # the CoT sequence has a random length\n",
    "        print(k)\n",
    "        for i in range(k + 2):\n",
    "            attn_mask = torch.cat((attn_mask, attn_ones), dim=1)\n",
    "            if i == 0 or i == k + 1: # process beginning of thought or end of thought token\n",
    "                seq = [biscuit_model.bot if i == 0 else biscuit_model.eot] * batch_size\n",
    "                inputs = biscuit_model.tokenizer(seq, return_tensors=\"pt\").to(biscuit_model.device)\n",
    "                args = {'input_ids': inputs.input_ids}\n",
    "            else: # process new continuous thought token\n",
    "                args = {'inputs_embeds': last_hidden_state}\n",
    "\n",
    "            outputs = biscuit_model.model(**args, attention_mask=attn_mask, past_key_values=kv_cache)\n",
    "            last_hidden_state = outputs.hidden_states[-1][:, -1:]\n",
    "            kv_cache = outputs.past_key_values\n",
    "\n",
    "        key_cache_copy = [t.clone() for t in kv_cache.key_cache]\n",
    "        value_cache_copy = [t.clone() for t in kv_cache.value_cache]\n",
    "\n",
    "        text_output = [' ' for _ in range(batch_size)]\n",
    "        next_token = text_output.copy()\n",
    "        temp_mask = attn_mask.clone()\n",
    "        for _ in range(50):\n",
    "            inputs = biscuit_model.tokenizer(next_token, return_tensors=\"pt\").to(biscuit_model.device)\n",
    "            temp_mask = torch.cat((temp_mask, attn_ones), dim=1)\n",
    "            outputs = biscuit_model.model(input_ids=inputs.input_ids, \n",
    "                                          attention_mask=temp_mask, \n",
    "                                          past_key_values=kv_cache)\n",
    "            next_token = biscuit_model.tokenizer.batch_decode(torch.multinomial(softmax(outputs.logits[:, -1]), 1))\n",
    "            text_output = [a + b for a, b in zip(text_output, next_token)]\n",
    "        for a, b in zip(text_output, segment):\n",
    "            print(\"model output:\", a)\n",
    "            print('real:', b)\n",
    "\n",
    "        kv_cache.key_cache = key_cache_copy\n",
    "        kv_cache.value_cache = value_cache_copy\n",
    "\n",
    "        # pad on the right side so that the CoT and the new input are contiguous\n",
    "        inputs = biscuit_model.tokenizer(segment, return_tensors=\"pt\", padding=True, \n",
    "                                padding_side='right').to(biscuit_model.device)\n",
    "        attn_mask = torch.cat((attn_mask, inputs.attention_mask), dim=1)\n",
    "        outputs = biscuit_model.model(input_ids=inputs.input_ids, attention_mask=attn_mask, past_key_values=kv_cache)\n",
    "        kv_cache = outputs.past_key_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biscuit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
