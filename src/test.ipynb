{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/BOSDYN/cjestin/Research/biscuit/biscuit-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "\n",
    "from gsm_dataset import GSMDataset, gsm_collate, gsm_prompt, sample\n",
    "from biscuit import Biscuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = f'checkpoints/epoch_9.pth'\n",
    "\n",
    "biscuit_model = Biscuit()\n",
    "biscuit_model.model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GSMDataset()\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "example_size = int(0.02 * len(dataset)) # reserve some data for few shot prompting\n",
    "test_size = len(dataset) - train_size - example_size\n",
    "\n",
    "train_dataset, example_dataset, test_dataset = random_split(dataset, [train_size, example_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=gsm_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=gsm_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Jimmy and Irene go shopping for clothes on a Tuesday, where senior citizens get a 10% discount on their purchases.  Jimmy picks out 3 shorts from the $15 rack.  Irene grabs 5 shirts from the $17 rack.  How much money do they give to the cashier?\n",
      "\n",
      "Answer: Jimmy’s shorts cost 3 x $15 = $\n",
      "Question: Emma has saved $230 in her bank account. She withdrew $60 to buy a new pair of shoes. The next week, she deposited twice as much money as she withdrew. How much is in her bank account now?\n",
      "\n",
      "Answer: Emma had $230 - $60 = $\n",
      "Question: Angela has a collection of 24 pieces of rare action figures. She sold off a quarter of them at the pawnshop and gave one-third of the remainder to her daughter. How many does she have left?\n",
      "\n",
      "Answer: One-quarter of 24 action figures is 24*(1/4) = \n",
      "Question: A mailman has to deliver 48 pieces of junk mail.  There are 8 houses on the block.  2 of the houses have white mailboxes and 3 have red mailboxes.  How many pieces of junk mail will each of those houses get?\n",
      "\n",
      "Answer: The mailman has 48 pieces of junk mail / 8 houses = \n",
      "1\n",
      "model output:  45.\n",
      "Irene’s shirts cost 5 x $17 = $95.\n",
      "Total cost for the clothes is $45 + $95 = $140.\n",
      "10% discount on the clothes is $140\n",
      "real: 45.\n",
      "Irene’s shirts cost 5 x $17 = $\n",
      "model output:  170 left in her bank account after she withdrew $60.\n",
      "The next week, she deposited $60 x 2 = $120.\n",
      "Hence, Emma's bank account now has a total of $120 +\n",
      "real: 170 left in her bank account after she withdrew $60.\n",
      "The next week, she deposited $60 x 2 = $\n",
      "model output:  6.\n",
      "She sold off 6 action figures so she has 24-6 = 18 left.\n",
      "One-third of 18 is 18*(1/3) = 6.\n",
      "She gave out 6 so she has\n",
      "real: 6\n",
      "She sold off 6 action figures so she has 24-6 = \n",
      "model output:  60 pieces per house.\n",
      "The houses with red mailboxes get 2 * 60 = 120 pieces of junk mail.\n",
      "The houses with white mailboxes get 3 * 60 = 180 pieces of junk\n",
      "real: 6 pieces per house.\n",
      "The houses with white mailboxes get 2 * 6 pieces = \n",
      "3\n",
      "model output:  95.\n",
      "Total cost for the clothes is $45 + $95 = $140.\n",
      "10% discount on the total cost is $140 x 10/100 = $14.\n",
      "They give\n",
      "real: 85.\n",
      "Total cost for the clothes is $45 + $85 = $\n",
      "model output:  120.\n",
      "Hence, the total amount of money in her bank account now is $170 - $120 = $70.\n",
      "#### 70\n",
      "#### 70\n",
      "#### 70\n",
      "#### 7\n",
      "real: 120.\n",
      "Hence, she now has a total of $170 + $120 = $\n",
      "model output:  18 still left\n",
      "One-third of 18 is 18*(1/3) = 6\n",
      "She gave out 6 so she has 18-6 = 12 left\n",
      "#### 12\n",
      "#### \n",
      "real: 18 left\n",
      "One-third of 18 is 18*(1/3) = \n",
      "model output:  12 pieces of junk mail.\n",
      "The houses with red mailboxes get 3 * 6 pieces = 18 pieces of junk mail.\n",
      "The houses with red and white mailboxes will get a total of 18 + 12 =\n",
      "real: 12 pieces of junk mail.\n",
      "The houses with red mailboxes get 3 * 6 pieces = \n",
      "1\n",
      "model output:  130.\n",
      "10% discount on the clothes is 10/100 x $130 = $13.\n",
      "They give the cashier $130 - $13 = $117.\n",
      "#### 1\n",
      "real: 130.\n",
      "10% discount on the clothes is $130 x 10% = $\n",
      "model output:  290 in her bank account.\n",
      "#### 290\n",
      "#### 290\n",
      "#### 290\n",
      "#### 290\n",
      "#### 290\n",
      "#### 290\n",
      "\n",
      "Question: Given the digits \n",
      "real: 290 in her bank account.\n",
      "#### 290\n",
      "model output:  6\n",
      "She gave out 6 so she has 18-6 = 12 left\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "\n",
      "Question: If $0 < h < k$ and $h +\n",
      "real: 6\n",
      "She gave out 6 so she has 18-6 = \n",
      "model output:  18 pieces of junk mail.\n",
      "The houses with red and white mailboxes will get a total of 12 + 18 = 20 pieces of junk mail.\n",
      "20/5 = 4 pieces of junk mail each\n",
      "####\n",
      "real: 18 pieces of junk mail.\n",
      "The houses with red and white mailboxes will get a total of 12 + 18 = \n",
      "5\n",
      "model output:  13.\n",
      "They give the cashier $130 - $13 = $117.\n",
      "#### 117\n",
      "#### 117\n",
      "#### 117\n",
      "\n",
      "Question: If instead of a best-friend swap,\n",
      "real: 13.\n",
      "They give the cashier $130 - $13 = $\n",
      "model output:  12 left\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "#### 12\n",
      "####\n",
      "real: 12 left\n",
      "#### 12\n",
      "model output:  30 pieces of junk mail.\n",
      "30/5 = 6 pieces of junk mail will each house get.\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "real: 30 pieces of junk mail.\n",
      "30/5 = \n",
      "1\n",
      "model output:  117.\n",
      "#### 117\n",
      "\n",
      "Question: Find the farthest number which if added to 4325 would give you circumference as the result in terms of the decimal number 12.102, if addition is\n",
      "real: 117.\n",
      "#### 117\n",
      "model output:  6 pieces of junk mail each\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "#### 6\n",
      "real: 6 pieces of junk mail each\n",
      "#### 6\n"
     ]
    }
   ],
   "source": [
    "COT_MAX_LENGTH = 6\n",
    "segments, keep_indices_lst = next(iter(train_loader))\n",
    "examples = sample(example_dataset, num_samples=4)\n",
    "prompt = gsm_prompt(examples)\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step 0: just process the first segment without decoding the next token\n",
    "    for seg in segments[0]:\n",
    "        print(seg)\n",
    "    first_segment = [prompt + segment for segment in segments[0]]\n",
    "    inputs = biscuit_model.tokenizer(first_segment, return_tensors=\"pt\", padding=True).to(biscuit_model.device)\n",
    "    outputs = biscuit_model.model(**inputs)\n",
    "    kv_cache = outputs.past_key_values\n",
    "    attn_mask = inputs.attention_mask\n",
    "\n",
    "    # continuous CoT loop: produce CoT -> use it to predict next segment -> repeat\n",
    "    for segment, keep_indices in zip(segments[1:], keep_indices_lst):\n",
    "        # Step 1: drop sequences that are done\n",
    "        kv_cache.batch_select_indices(keep_indices)\n",
    "        attn_mask = attn_mask[keep_indices]\n",
    "        batch_size = keep_indices.shape[0]\n",
    "        attn_ones = torch.ones(batch_size, 1, dtype=int).to(biscuit_model.device)\n",
    "\n",
    "\n",
    "        # Step 2: then autoregressively predict a continuous chain of thought sequence\n",
    "        last_hidden_state = None\n",
    "        k = np.random.randint(1, COT_MAX_LENGTH + 1) # the CoT sequence has a random length\n",
    "        print(k)\n",
    "        for i in range(k + 2):\n",
    "            attn_mask = torch.cat((attn_mask, attn_ones), dim=1)\n",
    "            if i == 0 or i == k + 1: # process beginning of thought or end of thought token\n",
    "                seq = [biscuit_model.bot if i == 0 else biscuit_model.eot] * batch_size\n",
    "                inputs = biscuit_model.tokenizer(seq, return_tensors=\"pt\").to(biscuit_model.device)\n",
    "                args = {'input_ids': inputs.input_ids}\n",
    "            else: # process new continuous thought token\n",
    "                args = {'inputs_embeds': last_hidden_state}\n",
    "\n",
    "            outputs = biscuit_model.model(**args, attention_mask=attn_mask, past_key_values=kv_cache)\n",
    "            last_hidden_state = outputs.hidden_states[-1][:, -1:]\n",
    "            kv_cache = outputs.past_key_values\n",
    "\n",
    "        key_cache_copy = [t.clone() for t in kv_cache.key_cache]\n",
    "        value_cache_copy = [t.clone() for t in kv_cache.value_cache]\n",
    "\n",
    "        text_output = [' ' for _ in range(batch_size)]\n",
    "        next_token = text_output.copy()\n",
    "        temp_mask = attn_mask.clone()\n",
    "        for _ in range(50):\n",
    "            inputs = biscuit_model.tokenizer(next_token, return_tensors=\"pt\").to(biscuit_model.device)\n",
    "            temp_mask = torch.cat((temp_mask, attn_ones), dim=1)\n",
    "            outputs = biscuit_model.model(input_ids=inputs.input_ids, \n",
    "                                          attention_mask=temp_mask, \n",
    "                                          past_key_values=kv_cache)\n",
    "            next_token = biscuit_model.tokenizer.batch_decode(torch.multinomial(softmax(outputs.logits[:, -1]), 1))\n",
    "            text_output = [a + b for a, b in zip(text_output, next_token)]\n",
    "        for a, b in zip(text_output, segment):\n",
    "            print(\"model output:\", a)\n",
    "            print('real:', b)\n",
    "\n",
    "        kv_cache.key_cache = key_cache_copy\n",
    "        kv_cache.value_cache = value_cache_copy\n",
    "\n",
    "        # pad on the right side so that the CoT and the new input are contiguous\n",
    "        inputs = biscuit_model.tokenizer(segment, return_tensors=\"pt\", padding=True, \n",
    "                                padding_side='right').to(biscuit_model.device)\n",
    "        attn_mask = torch.cat((attn_mask, inputs.attention_mask), dim=1)\n",
    "        outputs = biscuit_model.model(input_ids=inputs.input_ids, attention_mask=attn_mask, past_key_values=kv_cache)\n",
    "        kv_cache = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbiscuit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    700\u001b[0m )\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:405\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `past_key_values` should be either a `Cache` object or `None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mand\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m DynamicCache()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Research/biscuit/biscuit-env/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "biscuit_model.model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @torch.no_grad()\n",
      "    def generate(\n",
      "        self,\n",
      "        inputs: Optional[torch.Tensor] = None,\n",
      "        generation_config: Optional[GenerationConfig] = None,\n",
      "        logits_processor: Optional[LogitsProcessorList] = None,\n",
      "        stopping_criteria: Optional[StoppingCriteriaList] = None,\n",
      "        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n",
      "        synced_gpus: Optional[bool] = None,\n",
      "        assistant_model: Optional[\"PreTrainedModel\"] = None,\n",
      "        streamer: Optional[\"BaseStreamer\"] = None,\n",
      "        negative_prompt_ids: Optional[torch.Tensor] = None,\n",
      "        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n",
      "        use_model_defaults: Optional[bool] = None,\n",
      "        custom_generate: Optional[str] = None,\n",
      "        **kwargs,\n",
      "    ) -> Union[GenerateOutput, torch.LongTensor]:\n",
      "        r\"\"\"\n",
      "\n",
      "        Generates sequences of token ids for models with a language modeling head.\n",
      "\n",
      "        <Tip warning={true}>\n",
      "\n",
      "        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "\n",
      "        For an overview of generation strategies and code examples, check out the [following\n",
      "        guide](../generation_strategies).\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "        Parameters:\n",
      "            inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "                The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "                method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "                should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "                `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "            generation_config ([`~generation.GenerationConfig`], *optional*):\n",
      "                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "                passed to generate matching the attributes of `generation_config` will override them. If\n",
      "                `generation_config` is not provided, the default will be used, which has the following loading\n",
      "                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "                default values, whose documentation should be checked to parameterize generation.\n",
      "            logits_processor (`LogitsProcessorList`, *optional*):\n",
      "                Custom logits processors that complement the default logits processors built from arguments and\n",
      "                generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. This feature is intended for advanced users.\n",
      "            stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "                Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "                generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "                generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "                sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "                intended for advanced users.\n",
      "            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "                Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "            synced_gpus (`bool`, *optional*):\n",
      "                Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n",
      "                to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n",
      "                deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n",
      "            assistant_model (`PreTrainedModel`, *optional*):\n",
      "                An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "                same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistant model\n",
      "                is much faster than running generation with the model you're calling generate from. As such, the\n",
      "                assistant model should be much smaller.\n",
      "            streamer (`BaseStreamer`, *optional*):\n",
      "                Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "                through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "            negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "                size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "            negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "                Attention_mask for `negative_prompt_ids`.\n",
      "            use_model_defaults (`bool`, *optional*):\n",
      "                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n",
      "                generation configuration (`model.generation_config`), as opposed to the global defaults\n",
      "                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n",
      "                `True`.\n",
      "            custom_generate (`str`, *optional*):\n",
      "                A string containing the name of a huggingface.co repository. If provided, the custom `generate`\n",
      "                function defined in that reposity's `custom_generate/generate.py` file will be executed instead of the\n",
      "                standard `generate` method. Note that the logic is for generation is entirely defined in that\n",
      "                repository, and the return type may be different from the standard `generate` method.\n",
      "            kwargs (`Dict[str, Any]`, *optional*):\n",
      "                Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "                forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "                specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "\n",
      "        Return:\n",
      "            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "            or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "\n",
      "                If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                    - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "\n",
      "                If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "                [`~utils.ModelOutput`] types are:\n",
      "\n",
      "                    - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                    - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "        \"\"\"\n",
      "        # 0. If requested, load an arbitrary generation recipe from the Hub and run it instead\n",
      "        if custom_generate is not None:\n",
      "            trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n",
      "            # Get all `generate` arguments in a single variable. Custom functions are responsible for handling them:\n",
      "            # they receive the same inputs as `generate`, only with `model` instead of `self`. They can access to\n",
      "            # methods from `GenerationMixin` through `model`.\n",
      "            global_keys_to_exclude = {\"self\", \"kwargs\"}\n",
      "            generate_arguments = {key: value for key, value in locals().items() if key not in global_keys_to_exclude}\n",
      "            generate_arguments.update(kwargs)\n",
      "\n",
      "            custom_generate_function = self.load_custom_generate(\n",
      "                custom_generate, trust_remote_code=trust_remote_code, **kwargs\n",
      "            )\n",
      "            return custom_generate_function(model=self, **generate_arguments)\n",
      "\n",
      "        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n",
      "        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n",
      "        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n",
      "\n",
      "        generation_config, model_kwargs = self._prepare_generation_config(\n",
      "            generation_config, use_model_defaults, **kwargs\n",
      "        )\n",
      "        self._validate_model_kwargs(model_kwargs.copy())\n",
      "        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n",
      "\n",
      "        # 2. Set generation parameters if not already defined\n",
      "        if synced_gpus is None:\n",
      "            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n",
      "\n",
      "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
      "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
      "\n",
      "        accepts_attention_mask = \"attention_mask\" in set(inspect.signature(self.forward).parameters.keys())\n",
      "        requires_attention_mask = \"encoder_outputs\" not in model_kwargs\n",
      "        kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n",
      "\n",
      "        # 3. Define model inputs\n",
      "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n",
      "            inputs, generation_config.bos_token_id, model_kwargs\n",
      "        )\n",
      "        batch_size = inputs_tensor.shape[0]\n",
      "\n",
      "        device = inputs_tensor.device\n",
      "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
      "\n",
      "        # decoder-only models must use left-padding for batched generation.\n",
      "        if not self.config.is_encoder_decoder:\n",
      "            # If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\n",
      "            # Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\n",
      "            if (\n",
      "                generation_config._pad_token_tensor is not None\n",
      "                and batch_size > 1\n",
      "                and len(inputs_tensor.shape) == 2\n",
      "                and torch.sum(inputs_tensor[:, -1] == generation_config._pad_token_tensor) > 0\n",
      "            ):\n",
      "                logger.warning(\n",
      "                    \"A decoder-only architecture is being used, but right-padding was detected! For correct \"\n",
      "                    \"generation results, please set `padding_side='left'` when initializing the tokenizer.\"\n",
      "                )\n",
      "\n",
      "        # 4. Define other model kwargs\n",
      "        # decoder-only models with inputs_embeds forwarding must use caching (otherwise we can't detect whether we are\n",
      "        # generating the first new token or not, and we only want to use the embeddings for the first new token)\n",
      "        if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n",
      "            generation_config.use_cache = True\n",
      "\n",
      "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:\n",
      "            model_kwargs[\"attention_mask\"] = self._prepare_attention_mask_for_generation(\n",
      "                inputs_tensor, generation_config, model_kwargs\n",
      "            )\n",
      "        elif kwargs_has_attention_mask:\n",
      "            # TODO (joao): generalize this check with other types of inputs\n",
      "            if model_input_name == \"input_ids\" and len(model_kwargs[\"attention_mask\"].shape) > 2:\n",
      "                raise ValueError(\"`attention_mask` passed to `generate` must be 2D.\")\n",
      "\n",
      "        if self.config.is_encoder_decoder and \"encoder_outputs\" not in model_kwargs:\n",
      "            # if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\n",
      "            model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                inputs_tensor, model_kwargs, model_input_name, generation_config\n",
      "            )\n",
      "\n",
      "        # 5. Prepare `input_ids` which will be used for auto-regressive generation\n",
      "        if self.config.is_encoder_decoder:\n",
      "            input_ids, model_kwargs = self._prepare_decoder_input_ids_for_generation(\n",
      "                batch_size=batch_size,\n",
      "                model_input_name=model_input_name,\n",
      "                model_kwargs=model_kwargs,\n",
      "                decoder_start_token_id=generation_config._decoder_start_token_tensor,\n",
      "                device=inputs_tensor.device,\n",
      "            )\n",
      "        else:\n",
      "            input_ids = inputs_tensor if model_input_name == \"input_ids\" else model_kwargs.pop(\"input_ids\")\n",
      "\n",
      "        if generation_config.token_healing:\n",
      "            input_ids = self.heal_tokens(input_ids, tokenizer)\n",
      "\n",
      "        if streamer is not None:\n",
      "            streamer.put(input_ids.cpu())\n",
      "\n",
      "        # 6. Prepare `max_length` depending on other stopping criteria.\n",
      "        input_ids_length = input_ids.shape[1]\n",
      "        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n",
      "        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n",
      "        generation_config = self._prepare_generated_length(\n",
      "            generation_config=generation_config,\n",
      "            has_default_max_length=has_default_max_length,\n",
      "            has_default_min_length=has_default_min_length,\n",
      "            model_input_name=model_input_name,\n",
      "            inputs_tensor=inputs_tensor,\n",
      "            input_ids_length=input_ids_length,\n",
      "        )\n",
      "\n",
      "        # If the model supports `logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n",
      "        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n",
      "        # dynamically overrides this value as it can need more than the last token logits\n",
      "        if self._supports_logits_to_keep() and \"logits_to_keep\" not in model_kwargs:\n",
      "            model_kwargs[\"logits_to_keep\"] = 1\n",
      "\n",
      "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "\n",
      "        # 7. Prepare the cache.\n",
      "        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n",
      "        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n",
      "        # - `max_length`, prepared above, is used to determine the maximum cache length\n",
      "        max_cache_length = generation_config.max_length - 1\n",
      "        if (\n",
      "            inputs_tensor.shape[1] != input_ids_length\n",
      "            and model_input_name == \"inputs_embeds\"\n",
      "            and not self.config.is_encoder_decoder\n",
      "        ):\n",
      "            max_cache_length += inputs_tensor.shape[1]\n",
      "        self._prepare_cache_for_generation(\n",
      "            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device\n",
      "        )\n",
      "\n",
      "        # 8. determine generation mode\n",
      "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
      "\n",
      "        if streamer is not None and (generation_config.num_beams > 1):\n",
      "            raise ValueError(\n",
      "                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n",
      "            )\n",
      "\n",
      "        if self.device.type != input_ids.device.type:\n",
      "            warnings.warn(\n",
      "                \"You are calling .generate() with the `input_ids` being on a device type different\"\n",
      "                f\" than your model's device. `input_ids` is on {input_ids.device.type}, whereas the model\"\n",
      "                f\" is on {self.device.type}. You may experience unexpected behaviors or slower generation.\"\n",
      "                \" Please make sure that you have put `input_ids` to the\"\n",
      "                f\" correct device by calling for example input_ids = input_ids.to('{self.device.type}') before\"\n",
      "                \" running `.generate()`.\",\n",
      "                UserWarning,\n",
      "            )\n",
      "\n",
      "        # 9. prepare logits processors and stopping criteria\n",
      "        prepared_logits_processor = self._get_logits_processor(\n",
      "            generation_config=generation_config,\n",
      "            input_ids_seq_length=input_ids_length,\n",
      "            encoder_input_ids=inputs_tensor,\n",
      "            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
      "            logits_processor=logits_processor,\n",
      "            device=inputs_tensor.device,\n",
      "            model_kwargs=model_kwargs,\n",
      "            negative_prompt_ids=negative_prompt_ids,\n",
      "            negative_prompt_attention_mask=negative_prompt_attention_mask,\n",
      "        )\n",
      "        prepared_stopping_criteria = self._get_stopping_criteria(\n",
      "            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n",
      "        )\n",
      "\n",
      "        # Set model_kwargs `use_cache` so we can use it later in forward runs\n",
      "        model_kwargs[\"use_cache\"] = generation_config.use_cache\n",
      "\n",
      "        # 10. go into different generation modes\n",
      "        if generation_mode == GenerationMode.ASSISTED_GENERATION:\n",
      "            if generation_config.num_return_sequences > 1:\n",
      "                raise ValueError(\n",
      "                    \"num_return_sequences has to be 1 when doing assisted generate, \"\n",
      "                    f\"but is {generation_config.num_return_sequences}.\"\n",
      "                )\n",
      "            if batch_size > 1:\n",
      "                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n",
      "            if not model_kwargs[\"use_cache\"]:\n",
      "                raise ValueError(\"assisted generate requires `use_cache=True`\")\n",
      "            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n",
      "                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n",
      "            if self._is_stateful:\n",
      "                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n",
      "                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n",
      "                raise ValueError(\n",
      "                    f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\"\n",
      "                )\n",
      "\n",
      "            # 11. Get the candidate generator, given the parameterization\n",
      "            candidate_generator = self._get_candidate_generator(\n",
      "                generation_config=generation_config,\n",
      "                input_ids=input_ids,\n",
      "                inputs_tensor=inputs_tensor,\n",
      "                assistant_model=assistant_model,\n",
      "                logits_processor=logits_processor,\n",
      "                target_tokenizer=tokenizer,\n",
      "                assistant_tokenizer=assistant_tokenizer,\n",
      "                model_kwargs=model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 12. run assisted generate\n",
      "            result = self._assisted_decoding(\n",
      "                input_ids,\n",
      "                candidate_generator=candidate_generator,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "        elif generation_mode == GenerationMode.DOLA_GENERATION:\n",
      "            if self._is_stateful:\n",
      "                # DoLa decoding was not designed for stateful models, and would require some changes\n",
      "                raise ValueError(\n",
      "                    f\"dola decoding is not supported with stateful models, such as {self.__class__.__name__}\"\n",
      "                )\n",
      "            result = self._dola_decoding(\n",
      "                input_ids,\n",
      "                dola_layers=generation_config.dola_layers,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.CONTRASTIVE_SEARCH:\n",
      "            if not model_kwargs[\"use_cache\"]:\n",
      "                raise ValueError(\"Contrastive search requires `use_cache=True`\")\n",
      "            if self._is_stateful:\n",
      "                # Just like assisted generation, we need to be able to rollback to a previous state (see comment above)\n",
      "                raise ValueError(\n",
      "                    f\"contrastive search is not supported with stateful models, such as {self.__class__.__name__}\"\n",
      "                )\n",
      "\n",
      "            result = self._contrastive_search(\n",
      "                input_ids,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n",
      "            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_return_sequences,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n",
      "            result = self._sample(\n",
      "                input_ids,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                streamer=streamer,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n",
      "            # 11. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 12. run beam sample\n",
      "            result = self._beam_search(\n",
      "                input_ids,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.GROUP_BEAM_SEARCH:\n",
      "            # 11. prepare beam search scorer\n",
      "            beam_scorer = BeamSearchScorer(\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                num_beam_groups=generation_config.num_beam_groups,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 13. run beam search\n",
      "            result = self._group_beam_search(\n",
      "                input_ids,\n",
      "                beam_scorer,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        elif generation_mode == GenerationMode.CONSTRAINED_BEAM_SEARCH:\n",
      "            final_constraints = []\n",
      "            if generation_config.constraints is not None:\n",
      "                final_constraints = generation_config.constraints\n",
      "\n",
      "            if generation_config.force_words_ids is not None:\n",
      "\n",
      "                def typeerror():\n",
      "                    raise ValueError(\n",
      "                        \"`force_words_ids` has to either be a `List[List[List[int]]]` or `List[List[int]]` \"\n",
      "                        f\"of positive integers, but is {generation_config.force_words_ids}.\"\n",
      "                    )\n",
      "\n",
      "                if (\n",
      "                    not isinstance(generation_config.force_words_ids, list)\n",
      "                    or len(generation_config.force_words_ids) == 0\n",
      "                ):\n",
      "                    typeerror()\n",
      "\n",
      "                for word_ids in generation_config.force_words_ids:\n",
      "                    if isinstance(word_ids[0], list):\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any(not isinstance(token_ids, list) for token_ids in word_ids):\n",
      "                            typeerror()\n",
      "                        if any(\n",
      "                            any((not isinstance(token_id, int) or token_id < 0) for token_id in token_ids)\n",
      "                            for token_ids in word_ids\n",
      "                        ):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = DisjunctiveConstraint(word_ids)\n",
      "                    else:\n",
      "                        if not isinstance(word_ids, list) or len(word_ids) == 0:\n",
      "                            typeerror()\n",
      "                        if any((not isinstance(token_id, int) or token_id < 0) for token_id in word_ids):\n",
      "                            typeerror()\n",
      "\n",
      "                        constraint = PhrasalConstraint(word_ids)\n",
      "                    final_constraints.append(constraint)\n",
      "\n",
      "            # 11. prepare beam search scorer\n",
      "            constrained_beam_scorer = ConstrainedBeamSearchScorer(\n",
      "                constraints=final_constraints,\n",
      "                batch_size=batch_size,\n",
      "                num_beams=generation_config.num_beams,\n",
      "                device=inputs_tensor.device,\n",
      "                length_penalty=generation_config.length_penalty,\n",
      "                do_early_stopping=generation_config.early_stopping,\n",
      "                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n",
      "                max_length=generation_config.max_length,\n",
      "            )\n",
      "            # 12. interleave input_ids with `num_beams` additional sequences per batch\n",
      "            input_ids, model_kwargs = self._expand_inputs_for_generation(\n",
      "                input_ids=input_ids,\n",
      "                expand_size=generation_config.num_beams,\n",
      "                is_encoder_decoder=self.config.is_encoder_decoder,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "            # 13. run beam search\n",
      "            result = self._constrained_beam_search(\n",
      "                input_ids,\n",
      "                constrained_beam_scorer=constrained_beam_scorer,\n",
      "                logits_processor=prepared_logits_processor,\n",
      "                stopping_criteria=prepared_stopping_criteria,\n",
      "                generation_config=generation_config,\n",
      "                synced_gpus=synced_gpus,\n",
      "                **model_kwargs,\n",
      "            )\n",
      "\n",
      "        # Convert to legacy cache format if requested\n",
      "        if (\n",
      "            generation_config.return_legacy_cache is True\n",
      "            and hasattr(result, \"past_key_values\")\n",
      "            and getattr(result.past_key_values, \"to_legacy_cache\") is not None\n",
      "        ):\n",
      "            result.past_key_values = result.past_key_values.to_legacy_cache()\n",
      "        return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(biscuit_model.model.generate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [6331], 'attention_mask': [1]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biscuit_model.tokenizer('bot').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biscuit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
